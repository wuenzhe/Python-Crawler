{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05902167-2c12-4e7b-9811-505af3f77e8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Python爬虫教程\n",
    "\n",
    "课程来源：[Python超强爬虫8天速成（完整版）爬取各种网站数据实战案例](https://www.bilibili.com/video/BV1ha4y1H7sx?p=6&spm_id_from=pageDriver)\n",
    "\n",
    "## 使用场景\n",
    "\n",
    "通用爬虫：抓取系统的组成部分，抓取一张页面数据\n",
    "\n",
    "聚焦爬虫：在通用的基础上，抓取特定内容\n",
    "\n",
    "增量式爬虫：检测网站中数据更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffbfde9-d1a3-4794-a671-ecd6c58bac85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## http协议\n",
    "服务器与客户端进行数据交互的一种形式\n",
    "\n",
    "常用请求头信息\n",
    "\n",
    "- User-Agent：请求载体的身份标识\n",
    "\n",
    "- Connection：请求完毕后，是断开连接还是保持\n",
    "\n",
    "常用响应头协议\n",
    "- Content-Type：服务器响应客户端的数据类型\n",
    "\n",
    "## https协议\n",
    "\n",
    "- 安全的超文本协议\n",
    "\n",
    "加密方式\n",
    "\n",
    "- 对称密钥加密\n",
    "\n",
    "- 非对称密钥加密\n",
    "\n",
    "- 证书密钥加密"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e695c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21161a0-41b6-42ae-8d51-fd9a6eca5076",
   "metadata": {},
   "source": [
    "## requests模块\n",
    "\n",
    "作用：模拟浏览器发送请求\n",
    "\n",
    "如何使用：\n",
    "\n",
    "- 指定url\n",
    "\n",
    "- 发起请求\n",
    "\n",
    "- 获取响应数据\n",
    "\n",
    "- 持久化存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b19ed-7588-4cf1-9573-af2a5f3b7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#需求：爬取搜狗首页\n",
    "import requests\n",
    "#指定url\n",
    "url = 'https://www.sogou.com/'\n",
    "#发起请求\n",
    "#返回响应对象\n",
    "response = requests.get(url = url)\n",
    "#获取响应数据\n",
    "#获取字符串形式的响应数据\n",
    "page_text = response.text\n",
    "print(page_text)\n",
    "#持久化存储\n",
    "with open('./sougou.html', 'w', encoding ='utf-8') as fp:\n",
    "    fp.write(page_text)\n",
    "    print('爬取结束！')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0349ec9b-3f04-4f49-8df1-06db8b71e6e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 案例\n",
    "\n",
    "- 指定搜索结果页面\n",
    "\n",
    "- 破解百度翻译\n",
    "\n",
    "- 豆瓣电影分类排行榜\n",
    "\n",
    "- 肯德基餐厅查询\n",
    "\n",
    "- 国家药监局相关数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7377ebe9-adfa-47ae-a7f0-0a6feb011e3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 指定搜索结果页面\n",
    "\n",
    "### User-Agent伪装\n",
    "\n",
    "UA检测：\n",
    "\n",
    "门户网站的服务器检测对应的请求对应载体身份标识，\n",
    "若检测到载体的身份标识是某一浏览器，则为正常请求，\n",
    "若检测载体的身份标识为非浏览器，则异常，可能拒绝请求。\n",
    "\n",
    "查看方式：\n",
    "\n",
    "`Shift + Ctrl + I`打开浏览器开发者工具，点击网络，选中网站，点击标头，找到User-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3fa9b-0990-4de2-a9fd-6787ee73fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定搜索结果页面\n",
    "import requests\n",
    "#UA伪装\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30'\n",
    "}\n",
    "url = 'https://www.sogou.com/web'\n",
    "#处理url携带的参数：封装到字典中\n",
    "kw = input('enter a word:')\n",
    "param = {\n",
    "    'query' : kw\n",
    "}\n",
    "#对指定url发起请求是携带参数的，且请求过程中处理了参数\n",
    "response = requests.get(url = url, params = param, headers = headers)\n",
    "page_text = response.text\n",
    "filename = kw + '.html'\n",
    "with open(filename, 'w', encoding = 'utf-8') as fp:\n",
    "    fp.write(page_text)\n",
    "print(filename, '保存成功！')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d3bdc-ebc5-4652-88d8-9f3db217f13b",
   "metadata": {},
   "source": [
    "## 破解百度翻译\n",
    "\n",
    "刷新页面的一部分\n",
    "\n",
    "### 相关知识\n",
    "\n",
    "- post请求（携带了参数）\n",
    "\n",
    "- 响应数据是一组\n",
    "\n",
    "### 操作\n",
    "\n",
    "1. `Shift + Ctrl + I`打开开发者工具，点击网络，点击Fetch/XHR\n",
    "\n",
    "2. 点击名称，点击负载，找到显示关键词的组\n",
    "\n",
    "3. 返回标头，复制url\n",
    "\n",
    "4. 在标头中找到Content-Type，显示json则为json方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ed7ba-4978-47e0-b669-3350041c35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#破解百度翻译\n",
    "import requests\n",
    "import json\n",
    "post_url = 'https://fanyi.baidu.com/sug'\n",
    "#UA伪装\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30'\n",
    "}\n",
    "#post请求参数处理（与get相同）\n",
    "word = input('enter a word:')\n",
    "data = {\n",
    "    'kw' : word\n",
    "}\n",
    "#请求发送\n",
    "response = requests.post(url = post_url, data = data, headers = headers)\n",
    "#获取相应数据:json方法返回的是object(响应数据为json类型，才能使用)\n",
    "dic_obj = response.json()\n",
    "#持久化存储\n",
    "filename = word + '.json'\n",
    "fp = open(filename, 'w', encoding = 'utf-8')\n",
    "json.dump(dic_obj, fp = fp, ensure_ascii = False)\n",
    "print('over!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52583cb1",
   "metadata": {},
   "source": [
    "## 豆瓣电影分类排行榜\n",
    "\n",
    "下拉页面会进行部分刷新，如增加电影数量没滚动条位置变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#豆瓣电影分类排行榜\n",
    "import requests\n",
    "import json\n",
    "url = 'https://movie.douban.com/j/chart/top_list'\n",
    "param = {\n",
    "    'type' : '24',\n",
    "    'interval_id' : '100:90',\n",
    "    'action' : '',\n",
    "    'start' : '0', #从库中第几部电影开始取\n",
    "    'limit' : '20', #单次请求取出的个数\n",
    "}\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30'\n",
    "}\n",
    "response = requests.get(url = url, params = param, headers = headers)\n",
    "list_data = response.json()\n",
    "fp = open('./douban.json', 'w', encoding = 'utf-8')\n",
    "json.dump(list_data, fp = fp, ensure_ascii = False)\n",
    "print('over!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf8b22",
   "metadata": {},
   "source": [
    "## 作业：肯德基餐厅查询\n",
    "\n",
    "重点：\n",
    "\n",
    "1. 确定网址的url\n",
    "\n",
    "2. 确定post请求还是get请求\n",
    "\n",
    "3. 确定响应数据是text还是json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#作业：肯德基餐厅查询\n",
    "import requests\n",
    "url = 'https://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword'\n",
    "word = input('enter a city:')\n",
    "data = {\n",
    "    'cname' : '',\n",
    "    'pid' : '',\n",
    "    'keyword' : word,\n",
    "    'pageIndex' : '1',\n",
    "    'pageSize' : '10',\n",
    "}\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30'\n",
    "}\n",
    "response = requests.post(url = url, data = data, headers = headers)\n",
    "page_text = response.text\n",
    "filename = word + '.html'\n",
    "with open(filename, 'w', encoding = 'utf-8') as fp:\n",
    "    fp.write(page_text)\n",
    "print(filename, '保存成功！')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e4e4b",
   "metadata": {},
   "source": [
    "## 国家药监局基于化妆品生产许可证相关数据\n",
    "\n",
    "网址：[化妆品生产许可信息管理平台](http://scxk.nmpa.gov.cn:81/xk/)\n",
    "\n",
    "需求：需要爬取企业的具体信息，非主业列表\n",
    "\n",
    "并不一定能直接从地址栏中请求得到\n",
    "\n",
    "可能是**动态加载**出来的数据\n",
    "\n",
    "对应企业的数据是通过ajax动态请求得到的\n",
    "\n",
    "**<font color=red>未解决</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabcb485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#国家药监局基于化妆品生产许可证相关数据\n",
    "import requests\n",
    "url = 'http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList'\n",
    "data = {\n",
    "    'on': 'true',\n",
    "    'page': '1',\n",
    "    'pageSize': '15',\n",
    "    'productName': '',\n",
    "    'conditionType': '1',\n",
    "    'applyname': '',\n",
    "    'applysn': '',\n",
    "}\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30'\n",
    "}\n",
    "id_list = []\n",
    "json_ids = requests.post(url = url, headers = headers, data = data)\n",
    "for dic in json_ids['list']:\n",
    "    id_list.append(dic['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec68eda",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ac2239",
   "metadata": {},
   "source": [
    "## 数据解析\n",
    "\n",
    "聚焦爬虫：爬取页面指定内容\n",
    "\n",
    "分类：正则、bs4、**xpath**\n",
    "\n",
    "原理：\n",
    "\n",
    "- 解析局部文本内容在标签之间或标签对应的属性中存储\n",
    "\n",
    "- 指定标签进行定位\n",
    "\n",
    "- 标签或标签属性中存储的数据值进行提取（解析）\n",
    "\n",
    "流程：\n",
    "\n",
    "1. 指定url\n",
    "\n",
    "2. 发起请求\n",
    "\n",
    "3. 获取相应数据\n",
    "\n",
    "4. 数据解析\n",
    "\n",
    "5. 持久化存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece24754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬取图片数据\n",
    "import requests\n",
    "url = 'http://mari.hzau.edu.cn/__local/B/C9/4C/40EE6E9C872EBEF7B0E864D8540_34AE630B_15EF3.jpg'\n",
    "#content返回二进制图像数据\n",
    "#text字符串，content二进制，json对象\n",
    "img_data = requests.get(url = url).content\n",
    "with open('./image.jpg', 'wb') as fp:\n",
    "    fp.write(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a736b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬取所有图片数据\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "#创建文件夹\n",
    "if not os.path.exists('./marilibs'):\n",
    "    os.mkdir('./marilibs')\n",
    "#爬取页面信息\n",
    "url = 'http://mari.hzau.edu.cn/info/1003/3711.htm'\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30'\n",
    "}\n",
    "page_text = requests.post(url = url, headers = headers).text\n",
    "#获取图像数据\n",
    "#正则表达式\n",
    "ex = '<p style=\"text-align: center\"><img src=\"(.*?)\" width'\n",
    "img_src_list = re.findall(ex, page_text, re.S)\n",
    "print(img_src_list)\n",
    "#拼接图片地址\n",
    "for src in img_src_list:\n",
    "    src = 'http://mari.hzau.edu.cn' + src\n",
    "    img_data = requests.get(url = src, headers = headers).content\n",
    "#存储图片\n",
    "    img_name = src.split('/')[-1]\n",
    "    imgPath = './marilibs/' + img_name\n",
    "    with open(imgPath, 'wb') as fp:\n",
    "        fp.write(img_data)\n",
    "        print(img_name, '下载成功！')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c54585",
   "metadata": {},
   "source": [
    "## 分页爬取\n",
    "\n",
    "url当中包含通用，也包含页码参数，通过循环将页码拼接到通用url当中进行循环\n",
    "\n",
    "网页来源：[【壁纸】抱图吱声 壁纸头像各种类型](https://jump2.bdimg.com/p/4378860308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55dce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬取贴吧壁纸图片数据\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "#创建文件夹\n",
    "if not os.path.exists('./wallpapers'):\n",
    "    os.mkdir('./wallpapers')\n",
    "#UA伪装\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30'\n",
    "}\n",
    "#设置通用url模板\n",
    "url = 'https://jump2.bdimg.com/p/4378860308?pn=%d'\n",
    "pageNum = 1\n",
    "for pageNum in range(1, 5):\n",
    "    new_url = format(url%pageNum) \n",
    "    page_text = requests.post(url = new_url, headers = headers).text\n",
    "#获取图像数据\n",
    "#正则表达式\n",
    "    ex = '<img class=\"BDE_Image\" src=\"(.*?)\" size'\n",
    "    img_src_list = re.findall(ex, page_text, re.S)\n",
    "    print(img_src_list)\n",
    "#拼接图片地址\n",
    "    for src in img_src_list:\n",
    "        src = src\n",
    "        img_data = requests.get(url = src, headers = headers).content\n",
    "#存储图片\n",
    "        img_name = src.split('/')[-1]\n",
    "        imgPath = './wallpapers/' + img_name\n",
    "        with open(imgPath, 'wb') as fp:\n",
    "            fp.write(img_data)\n",
    "            print(img_name, '下载成功！')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8db328",
   "metadata": {},
   "source": [
    "## bs4解析\n",
    "\n",
    "Python独有的解析方式\n",
    "\n",
    "原理：\n",
    "- 实例化一个BeautifulSoup对象，并将页面源码加载到该对象中\n",
    "\n",
    "- 通过调用BeautifulSoup对象中的相关属性或方法对标签进行定位和数据提取\n",
    "\n",
    "安装：`bs4`，`lxml`\n",
    "\n",
    "对象的实例化：\n",
    "\n",
    "- 将本地html文档数据加载到对象当中\n",
    "    \n",
    "```python\n",
    "fp = open('./test.html', 'r', encoding = 'utf-8')\n",
    "soup = BeautifulSoup(fp, 'lxml')\n",
    "```\n",
    "\n",
    "- 从网上获取源码加载到该对象中\n",
    "    \n",
    "```python\n",
    "page_text = response.text    \n",
    "soup = BeautifulSoup(page_text, 'lxml')\n",
    "```\n",
    "\n",
    "提供的用于数据解析的方法和属性：\n",
    "\n",
    "- `soup.tagName`返回的是文档中第一个出现的tagName标签\n",
    "\n",
    "- `soup.find`\n",
    "\n",
    "    - 等同于soup.tagName\n",
    "    \n",
    "    - 属性定位：`soup.find('div', class_/id/attr = 'song')`\n",
    "    \n",
    "- `soup.find_all(tagName)`返回符合要求的所有标签\n",
    "\n",
    "- `soup.slelct`\n",
    "\n",
    "    - `select('某种选择器(id, class, 标签...选择器)')`，返回的是一个列表\n",
    "    \n",
    "    - `soup.select('.tang > ul > li > a')[0])`，层级选择器，大于号表示一个层级\n",
    "    \n",
    "    -  `print(soup.select('.tang > ul a'))`，空格表示的是多个层级\n",
    "\n",
    "获取标签之间文本数据：\n",
    "\n",
    "- `soup.a.text/string/get_text()`\n",
    "\n",
    "- `text和get_text()`可以获取某一标签下所有的文本内容\n",
    "\n",
    "- `string`只可以获取该标签下的直系文本\n",
    "\n",
    "获取标签中属性值：\n",
    "\n",
    "- `soup.a['href']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7139fd42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#将本地html文档数据加载到对象当中\n",
    "from bs4 import BeautifulSoup\n",
    "fp = open('./test.html', 'r', encoding = 'utf-8')\n",
    "soup = BeautifulSoup(fp, 'lxml')\n",
    "#print(soup)\n",
    "#print(soup.a) #soup.tagName返回的是html中第一个出现的a标签\n",
    "#print(soup.div)\n",
    "#print(soup.find('div')) #等同于soup.div\n",
    "#print(soup.find('div', class_ = 'song')) #属性定位\n",
    "#print(soup.find_all('a'))\n",
    "#print(soup.select('.tang'))\n",
    "#print(soup.select('.tang > ul > li > a')[0])\n",
    "#print(soup.select('.tang > ul a')[0].get_text())\n",
    "#print(soup.select('.tang > ul a')[0]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf738c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬取三国演义小说所有章节的标题和内容\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#爬取首页页面数据\n",
    "url = 'https://www.shicimingju.com/book/sanguoyanyi.html'\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.30'\n",
    "}\n",
    "page_text = requests.get(url = url, headers = headers).content\n",
    "#解析章节标题与详情页\n",
    "soup = BeautifulSoup(page_text, 'lxml')\n",
    "li_list = soup.select('.book-mulu > ul > li')\n",
    "fp = open('./sanguo.txt', 'w', encoding = 'utf-8')\n",
    "for li in li_list:\n",
    "    title = li.a.string\n",
    "    detail_url = 'https://www.shicimingju.com' + li.a['href']\n",
    "    #对详情页发起请求\n",
    "    detail_page_text = requests.get(url = detail_url, headers = headers).content\n",
    "    #解析详情页内容\n",
    "    detail_soup = BeautifulSoup(detail_page_text, 'lxml')\n",
    "    div_tag = detail_soup.find('div', class_ = 'chapter_content')\n",
    "    content = div_tag.text\n",
    "    fp.write(title + ':' +content + '\\n')\n",
    "    print(title, '爬取成功！')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20d26b",
   "metadata": {},
   "source": [
    "## xpath解析\n",
    "\n",
    "最常用且便捷的解析方式，具备通用性\n",
    "\n",
    "步骤：\n",
    "\n",
    "- 实例化etree对象，需要将被解析的页面源码加载到该对象中\n",
    "\n",
    "- 调用etree对象中的xpath方法结合着xpath、表达式实现标签定位和内容捕获\n",
    "\n",
    "安装`lxml`\n",
    "\n",
    "原理：\n",
    "\n",
    "- 将本地html加载到etree对象中\n",
    "\n",
    "    etree.parse(filpath)\n",
    "    \n",
    "- 将互联网中的源码数据加载\n",
    "\n",
    "    etree.HTML(\"page_text')\n",
    "\n",
    "- xpath('xpath表达式')\n",
    "\n",
    "    - 从根节点开始定位，用单斜杠表示切换层级\n",
    "    \n",
    "    - 双斜杠表示多个层级，可以从任意位置开始定位\n",
    "    \n",
    "    - 属性定位：`//tag[@attrName = \"attrValue\"]`\n",
    "    \n",
    "    - 索引定位：`//div[@class = \"song\"]/p[3]`\n",
    "    \n",
    "    - 取文本：\n",
    "        \n",
    "        - `text()`获取的是标签中直系的文本内容\n",
    "    \n",
    "        - `//text()`标签中非直系的文本内容（所有的文本内容）\n",
    "    \n",
    "    - 取属性：`/attrName`相当于`img/src`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030aac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "#实例化一个对象\n",
    "tree = etree.parse('./test.html')\n",
    "# r = tree.xpath('/html/body/div') #xpath只能按层级进行定位\n",
    "# r = tree.xpath('/html//div')\n",
    "# r = tree.xpath('//div')\n",
    "# r = tree.xpath('//div[@class = \"song\"]')\n",
    "# r = tree.xpath('//div[@class = \"song\"]/p[3]')\n",
    "# r = tree.xpath('//div[@class = \"tang\"]//li[5]/a/text()')[0] #取文本中的第一个元素\n",
    "# r = tree.xpath('//li[7]//text()')[0]\n",
    "#r = tree.xpath('//div[@class = \"song\"]/img/@src')\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬取58同城二手房信息\n",
    "import requests\n",
    "from lxml import etree\n",
    "#文本获取\n",
    "headers = {\n",
    "    'Uer-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36 Edg/102.0.1245.33'\n",
    "}\n",
    "url = 'https://www.58.com/ershoufang/'\n",
    "page_text = requests.get(url = url, headers = headers).text\n",
    "#数据解析\n",
    "tree = etree.HTML(page_text)\n",
    "tr_list = tree.xpath('//div[@class = \"cleft\"]//tr')\n",
    "for tr in tr_list:\n",
    "    tr.xpath('./td[2]/a/text()')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
